{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as p\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.model_selection as sms\n",
    "documents_categories = p.read_csv(\"/Users/astrachan/Desktop/outbrain/source_tables/documents_categories.csv\")\n",
    "#clicks_test = p.read_csv(\"/Users/astrachan/Desktop/outbrain/source_tables/clicks_test.csv\", dtype={\"display_id\": int, \"ad_id\": int}))\n",
    "#documents_meta = p.read_csv(\"/Users/astrachan/Desktop/outbrain/source_tables/documents_meta.csv\")\n",
    "#documents_entities = p.read_csv(\"/Users/astrachan/Desktop/outbrain/source_tables/documents_entities.csv\")\n",
    "promoted_content = p.read_csv(\"/Users/astrachan/Desktop/outbrain/source_tables/promoted_content.csv\", dtype = {\"ad_id\":int, \"document_id\":int, \"campaign_id\":int, \"advertiser_id\":int})\n",
    "#sample_submission = p.read_csv(\"/Users/astrachan/Desktop/outbrain/source_tables/sample_submission.csv\")\n",
    "documents_topics = p.read_csv(\"/Users/astrachan/Desktop/outbrain/source_tables/documents_topics.csv\")\n",
    "clicks_train = p.read_csv(\"/Users/astrachan/Desktop/outbrain/source_tables/clicks_train.csv\", dtype={\"display_id\": int, \"ad_id\": int, \"clicked\" : bool})\n",
    "events = p.read_csv(\"/Users/astrachan/Desktop/outbrain/source_tables/events.csv\", dtype={\"display_id\": int, \"uuid\": str, \"document_id\" : int, \"timestamp\" : int, \"platform\" : str, \"geo_location\" : str})\n",
    "#page_views = p.read_csv(\"/Users/astrachan/Desktop/outbrain/source_tables/page_views.csv\")\n",
    "#page_views_sample = p.read_csv(\"/Users/astrachan/Desktop/outbrain/source_tables/page_views_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#returns 2 tables, document on ad - doc-ad normalized score ; ad strength - ad normalized score\n",
    "#requires: base table of train (or validation similiar to train) with document id of display id merged (from events)\n",
    "def ad_on_doc_str(train):\n",
    "    train_ad_count_per_display = train.groupby(['display_id'])['display_id'].agg({'ads_on_doc' : 'count'}).reset_index()\n",
    "    train_temp = train.merge(train_ad_count_per_display, how = 'left', on = 'display_id')\n",
    "    del train_ad_count_per_display\n",
    "    document_on_ad = train_temp.groupby(['document_id','ad_id','ads_on_doc']).clicked.agg({'clicked' : 'sum', 'total' : 'count'}).reset_index()\n",
    "    document_on_ad['uni_chance'] = 1 / document_on_ad['ads_on_doc']\n",
    "    document_on_ad['clicked_percent'] = document_on_ad['clicked'] / document_on_ad['total']\n",
    "    document_on_ad['clicked_percent_normalized'] = (document_on_ad['clicked'] + 12 * document_on_ad['clicked_percent'].mean()) / (12 + document_on_ad['total'])\n",
    "    document_on_ad['likelihood_normalized'] = document_on_ad['clicked_percent_normalized'] / document_on_ad['uni_chance']\n",
    "    document_on_ad['like_mul_total_normalized'] = document_on_ad['likelihood_normalized'] * document_on_ad['total']\n",
    "    document_on_ad = document_on_ad.drop(document_on_ad.columns[[2,4,5,6,7,8]],axis = 1).groupby(['document_id','ad_id']).sum().reset_index()\n",
    "    document_on_ad['score'] = document_on_ad['like_mul_total_normalized'] / document_on_ad['total']\n",
    "    document_on_ad.drop(document_on_ad.columns[[2,3]],axis = 1, inplace=True)\n",
    "    document_on_ad.sort_values('score',inplace=True, ascending=False)\n",
    "    ad_strength = train_temp.groupby(['ad_id','ads_on_doc']).clicked.agg({'clicked' : 'sum', 'total' : 'count'}).reset_index()\n",
    "    ad_strength['uni_chance'] = 1 / ad_strength['ads_on_doc']\n",
    "    ad_strength['clicked_percent'] = ad_strength['clicked'] / ad_strength['total']\n",
    "    ad_strength['clicked_percent_normalized'] = (ad_strength['clicked'] + 12 * ad_strength['clicked_percent'].mean()) / (12 + ad_strength['total'])\n",
    "    ad_strength['likelihood_normalized'] = ad_strength['clicked_percent_normalized'] / ad_strength['uni_chance']\n",
    "    ad_strength['like_mul_total_normalized'] = ad_strength['likelihood_normalized'] * ad_strength['total']\n",
    "    ad_strength = ad_strength.drop(ad_strength.columns[[1,3,4,5,6,7]],axis = 1).groupby(['ad_id']).sum().reset_index()\n",
    "    ad_strength['score'] = ad_strength['like_mul_total_normalized'] / ad_strength['total']\n",
    "    ad_strength.drop(ad_strength.columns[[1,2]],axis = 1, inplace=True)\n",
    "    ad_strength.sort_values('score',inplace=True, ascending=False)\n",
    "    del train_temp\n",
    "    return document_on_ad, ad_strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#add document to clicks_train\n",
    "events.drop(events.columns[[1,3,4,5]],axis = 1,inplace=True)\n",
    "clicks_train = clicks_train.merge(events, how='left', on='display_id')\n",
    "del events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#1 - choose either 1 or 2\n",
    "#split to validation and train, choosing display_id by random\n",
    "display_ids = clicks_train.groupby(['display_id'])['display_id'].agg({'count' : 'count'}).reset_index().drop('count',axis = 1)\n",
    "validation_displays = display_ids.sample(frac = 0.1)\n",
    "clicks_train.reset_index(level=0, inplace=True)\n",
    "validation = validation_displays.merge(clicks_train, how = 'inner', on = 'display_id')\n",
    "train = clicks_train.drop(validation['index'])\n",
    "del clicks_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3484156, 4)\n",
      "(871686, 4)\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "#10% of full train for train, 2.5% validation, change (frac = 0.125) for different precent (4/5 train 1/5 validation)\n",
    "#split to validation and train, choosing display_id by random\n",
    "display_ids = clicks_train.groupby(['display_id'])['display_id'].agg({'count' : 'count'}).reset_index().drop('count',axis = 1)\n",
    "chosen_displays = display_ids.sample(frac = 0.05)\n",
    "train_displays , validation_displays = sms.train_test_split(chosen_displays, test_size = 0.2)\n",
    "validation = validation_displays.merge(clicks_train, how = 'inner', on = 'display_id')\n",
    "train = train_displays.merge(clicks_train, how = 'inner', on = 'display_id')\n",
    "print train.shape\n",
    "print validation.shape\n",
    "del clicks_train, train_displays, validation_displays, display_ids, chosen_displays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#proccess of one-hot on topics and categories\n",
    "#remove all confidence levels below 0.3, can decide on another criteria later on\n",
    "documents_categories = documents_categories[documents_categories.confidence_level > 0.3]\n",
    "documents_topics = documents_topics[documents_topics.confidence_level > 0.3]\n",
    "#drop confidence level\n",
    "documents_categories.drop('confidence_level',axis=1, inplace=True)\n",
    "documents_topics.drop('confidence_level',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare one-hot\n",
    "topic_dummies = p.get_dummies(documents_topics['topic_id'],prefix=\"_ti\")\n",
    "categories_dummies = p.get_dummies(documents_categories['category_id'],prefix=\"_ci\")\n",
    "topics = documents_topics.drop('topic_id',1).join(topic_dummies)\n",
    "topics = topics.groupby(['document_id']).sum().reset_index()\n",
    "categories = documents_categories.drop('category_id',1).join(categories_dummies)\n",
    "categories = categories.groupby(['document_id']).sum().reset_index()\n",
    "del documents_categories, categories_dummies, topic_dummies , documents_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ad_id  document_id  campaign_id  advertiser_id\n",
      "0      1         6614            1              7\n",
      "1      2       471467            2              7\n",
      "2      3         7692            3              7\n",
      "3      4       471471            2              7\n",
      "4      5       471472            2              7\n",
      "   document_id  _ci_1000  _ci_1100  _ci_1200  _ci_1202  _ci_1203  _ci_1204  \\\n",
      "0            1       0.0       0.0       0.0       0.0       0.0       0.0   \n",
      "1            2       0.0       0.0       0.0       0.0       0.0       0.0   \n",
      "2            3       0.0       0.0       0.0       0.0       0.0       0.0   \n",
      "3            4       0.0       0.0       0.0       0.0       0.0       0.0   \n",
      "4            5       0.0       0.0       0.0       0.0       0.0       0.0   \n",
      "\n",
      "   _ci_1205  _ci_1206  _ci_1207    ...     _ci_1913  _ci_1914  _ci_1915  \\\n",
      "0       0.0       0.0       0.0    ...          0.0       0.0       0.0   \n",
      "1       0.0       0.0       0.0    ...          0.0       0.0       0.0   \n",
      "2       0.0       0.0       0.0    ...          0.0       0.0       0.0   \n",
      "3       0.0       0.0       0.0    ...          0.0       0.0       0.0   \n",
      "4       0.0       0.0       0.0    ...          0.0       0.0       0.0   \n",
      "\n",
      "   _ci_2000  _ci_2002  _ci_2003  _ci_2004  _ci_2005  _ci_2006  _ci_2100  \n",
      "0       0.0       0.0       0.0       0.0       0.0       0.0       0.0  \n",
      "1       0.0       0.0       0.0       0.0       0.0       0.0       0.0  \n",
      "2       0.0       0.0       1.0       0.0       0.0       0.0       0.0  \n",
      "3       0.0       0.0       1.0       0.0       0.0       0.0       0.0  \n",
      "4       0.0       0.0       0.0       0.0       0.0       1.0       0.0  \n",
      "\n",
      "[5 rows x 96 columns]\n",
      "   document_id  _ti_0  _ti_1  _ti_2  _ti_3  _ti_4  _ti_5  _ti_6  _ti_7  _ti_8  \\\n",
      "0          157    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "1          159    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "2          161    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "3          650    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "4          664    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "\n",
      "    ...     _ti_290  _ti_291  _ti_292  _ti_293  _ti_294  _ti_295  _ti_296  \\\n",
      "0   ...         0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "1   ...         0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "2   ...         0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "3   ...         0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "4   ...         0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "\n",
      "   _ti_297  _ti_298  _ti_299  \n",
      "0      0.0      0.0      0.0  \n",
      "1      0.0      0.0      0.0  \n",
      "2      0.0      0.0      0.0  \n",
      "3      0.0      0.0      0.0  \n",
      "4      0.0      0.0      0.0  \n",
      "\n",
      "[5 rows x 296 columns]\n"
     ]
    }
   ],
   "source": [
    "print promoted_content.head()\n",
    "print categories.head()\n",
    "print topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create promoted, promoted content (ad information) with its' categories and topics\n",
    "#remove advertiser_id and campaign id\n",
    "promoted_content.drop(promoted_content.columns[[2,3]],axis = 1,inplace=True)\n",
    "promoted = promoted_content.merge(categories, how='left', on='document_id')\n",
    "promoted = promoted.merge(topics, how='left', on='document_id').drop('document_id', axis=1)\n",
    "del promoted_content\n",
    "#all Nulls are of documents with no topic or category, equals to 0\n",
    "promoted.fillna(0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_adoc, train_str = ad_on_doc_str(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#merge them into train and validation\n",
    "train = train.merge(train_adoc, how='left', on = ['document_id', 'ad_id'])\n",
    "train = train.merge(train_str, how='left', on = 'ad_id', suffixes= ('_adoc','_str'))\n",
    "validation = validation.merge(train_adoc, how = 'left', on = ['document_id', 'ad_id'])\n",
    "validation = validation.merge(train_str, how = 'left', on = 'ad_id', suffixes=('_adoc','_str'))\n",
    "del train_adoc, train_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(871686, 6)\n",
      "score_adoc nulls: 529542\n",
      "portion: 0.6074916885208664\n",
      "score_str nulls: 19123\n",
      "portion: 0.02193794554461125\n"
     ]
    }
   ],
   "source": [
    "#check ammount of missing data and fill\n",
    "print validation.shape\n",
    "doc_ad_null = validation.score_adoc.isnull().sum()\n",
    "str_null = validation.score_str.isnull().sum()\n",
    "print 'score_adoc nulls: ' + str(doc_ad_null)\n",
    "print 'portion: ' + repr(float(doc_ad_null) / validation.shape[0])\n",
    "print 'score_str nulls: ' + str(str_null)\n",
    "print 'portion: ' + repr(float(str_null) / validation.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#consider adding bit signaling the NAed ad_on_doc\n",
    "#check if median or mean are better (or min?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_adoc mean = 1.1258786409910577\n",
      "score_adoc median = 1.037802584372659\n",
      "score_str mean = 1.0209021240995964\n",
      "score_str median = 0.9156235753390366\n"
     ]
    }
   ],
   "source": [
    "#for filling NAs, check median mean and min\n",
    "print 'score_adoc mean = ' + repr(validation.score_adoc.mean())\n",
    "print 'score_adoc median = ' + repr(validation.score_adoc.median())\n",
    "print 'score_str mean = ' + repr(validation.score_str.mean())\n",
    "print 'score_str median = ' + repr(validation.score_str.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fill NAs, pick with which method\n",
    "validation.score_adoc = validation.score_adoc.fillna(validation.score_adoc.median())\n",
    "validation.score_str = validation.score_str.fillna(validation.score_str.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get predictors, dummy chunk just for label names, good for both 'not in chunks' and chunks\n",
    "chunk = train[:10].merge(promoted,how = 'left',on = 'ad_id')\n",
    "chunk = chunk.merge(categories, how='left', on='document_id',suffixes=('_ad', '_doc'))\n",
    "chunk = chunk.merge(topics, how='left', on='document_id',suffixes=('_ad', '_doc'))\n",
    "predictors=[x for x in train.columns if x not in ['index','display_id','ad_id','document_id','clicked']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1 - choose either 1 or 2\n",
    "#not in chunks\n",
    "#build classifier\n",
    "train = train.merge(categories, how = 'left', on = 'document_id')\n",
    "train = train.merge(topics, how = 'left', on = 'document_id')\n",
    "train = train.merge(promoted, how = 'left', on = 'ad_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#not in chunks\n",
    "#initiallize algorithm and fit\n",
    "alg = GradientBoostingClassifier(n_estimators = 500, max_depth=4, min_samples_split = 4, max_leaf_nodes = 4)\n",
    "#alg = RandomForestClassifier(random_state=1, n_estimators=3, min_samples_split=4, min_samples_leaf=2)\n",
    "alg.fit(train[predictors], train[\"clicked\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#not in chunks\n",
    "#run on validation\n",
    "validation = validation.merge(categories, how = 'left', on = 'document_id')\n",
    "validation = validation.merge(topics, how = 'left', on = 'document_id')\n",
    "validation = validation.merge(promoted, how = 'left', on = 'ad_id')\n",
    "predY=[]\n",
    "predY = list(alg.predict_proba(validation[predictors]).astype(float)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "#chunks - number of iterations for training\n",
    "print train.shape[0]/10**5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "#chunks\n",
    "#initiallize algorithm then fit it on chunks of size 10**5\n",
    "i = 0\n",
    "k = 3\n",
    "#alg = GradientBoostingClassifier(n_estimators = 10, max_depth=4, min_samples_split = 4, max_leaf_nodes = 4, warm_start=True)\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=3, min_samples_split=4, min_samples_leaf=2, warm_start=True)\n",
    "for chunk in np.array_split(train, train.shape[0]/10**5):\n",
    "    alg.n_estimators += k\n",
    "    chunk = chunk.merge(promoted,how = 'left',on = 'ad_id')\n",
    "    chunk = chunk.merge(categories, how='left', on='document_id',suffixes=('_ad', '_doc'))\n",
    "    chunk = chunk.merge(topics, how='left', on='document_id',suffixes=('_ad', '_doc'))\n",
    "    chunk.fillna(0.0, inplace=True)\n",
    "    alg.fit(chunk[predictors], chunk[\"clicked\"],)\n",
    "    i += 1\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save outputted tree with name [tree_name]\n",
    "tree_name = 'fourthTree_n_est_drop_no_topics'\n",
    "with open('/Users/astrachan/Desktop/outbrain/' + tree_name, 'wb') as f:\n",
    "    cPickle.dump(alg, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#chunks - number of iterations for validating\n",
    "print (validation.shape[0]/10**5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#chunks - run outputted tree on chunks of size 10**5 of validation\n",
    "predY=[]\n",
    "i = 0\n",
    "for chunk in np.array_split(validation[:10**6], validation[:10**6].shape[0]/10**5):\n",
    "    chunk = chunk.merge(promoted,how = 'left',on = 'ad_id')\n",
    "    chunk = chunk.merge(categories, how='left', on='document_id',suffixes=('_ad', '_doc'))\n",
    "    chunk = chunk.merge(topics, how='left', on='document_id',suffixes=('_ad', '_doc'))\n",
    "    chunk.fillna(0.0, inplace=True)\n",
    "    chunk_pred=list(alg.predict_proba(chunk[predictors]).astype(float)[:,1])\n",
    "    predY += chunk_pred\n",
    "    i += 1\n",
    "    print i\n",
    "predY = list(alg.predict_proba(validation[predictors]).astype(float)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check score\n",
    "predict = np.asarray(predY)\n",
    "val_copy = validation.copy()\n",
    "val_copy['predict'] = predict\n",
    "#delete predict? validation? chunk? chunk_pred? predictors? predY? bentaim lo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6920424300232756"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#portion of correct display_ids predictions\n",
    "max_rows = val_copy.groupby(['display_id'])['predict'].transform(max) == val_copy['predict']\n",
    "final = val_copy[max_rows]\n",
    "success = final[final['clicked'] == True]\n",
    "float(len(success)) / float(len(final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP: 0.816083156129\n"
     ]
    }
   ],
   "source": [
    "#function evaluating how far were you from predicting right\n",
    "val_copy.sort_values(['display_id', 'predict'], inplace=True, ascending=[True, False] )\n",
    "val_copy[\"seq\"] = np.arange(val_copy.shape[0])\n",
    "Y_seq = val_copy[val_copy.clicked == 1].seq.values\n",
    "Y_first = val_copy[['display_id', 'seq']].drop_duplicates(subset='display_id', keep='first').seq.values\n",
    "Y_ranks = Y_seq - Y_first\n",
    "score = np.mean(1.0 / (1.0 + Y_ranks))\n",
    "print(\"MAP: %.12f\" % score)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
